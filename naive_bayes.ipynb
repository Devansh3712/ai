{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes\n",
    "Probabilistic classifier based on baye's theorem with the naive assumption of independence between every pair of features. It's called naive because it simplifies the calculation by assuming that the presence of one feature is independent of the presence of any other feature.\n",
    "\n",
    "$P(y|x_1, x_2, ..., x_n) = \\frac{P(y) \\cdot P(x_1, x_2, ..., x_n|y)}{P(x_1, x_2, ..., x_n)}$\n",
    "\n",
    "$P(x_1, x_2, ..., x_n|y) = P(x_1|y) \\times P(x_2|y) \\times ... \\times P(x_n|y)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class NaiveBayes:\n",
    "    def fit(self, X: np.array, y: np.array) -> None:\n",
    "        n_samples, n_features = X.shape\n",
    "        self.classes = np.unique(y)\n",
    "        n_classes = len(self.classes)\n",
    "\n",
    "        self.mean = np.zeros((n_classes, n_features), dtype=np.float64)\n",
    "        self.var = np.zeros((n_classes, n_features), dtype=np.float64)\n",
    "        self.priors = np.zeros(n_classes, dtype=np.float64)\n",
    "\n",
    "        for index, _class in enumerate(self.classes):\n",
    "            X_c = X[y == _class]\n",
    "            self.mean[index,:] = X_c.mean(axis=0)\n",
    "            self.var[index,:] = X_c.var(axis=0)\n",
    "            self.priors[index] = X_c.shape[0] / float(n_samples)\n",
    "\n",
    "    def _probability_density(self, index, x: np.array) -> np.array:\n",
    "        \"\"\"Probability density function of a normal distribution\"\"\"\n",
    "        mean = self.mean[index]\n",
    "        var = self.var[index]\n",
    "        numerator = np.exp(-((x - mean) ** 2) / (2 * var))\n",
    "        denominator = np.sqrt(2 * np.pi * var)\n",
    "        return numerator / denominator\n",
    "\n",
    "    def _predict(self, x: np.array) -> int:\n",
    "        posteriors = []\n",
    "        n_classes = len(self.classes)\n",
    "\n",
    "        for i in range(n_classes):\n",
    "            prior = np.log(self.priors[i])\n",
    "            posterior = np.sum(np.log(self._probability_density(i, x)))\n",
    "            posterior += prior\n",
    "            posteriors.append(posterior)\n",
    "\n",
    "        return self.classes[np.argmax(posteriors)]\n",
    "\n",
    "    def predict(self, X: np.array) -> list[int]:\n",
    "        return [self._predict(x) for x in X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.835"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, y = datasets.make_classification(\n",
    "    n_samples=1000, n_features=10, n_classes=2, random_state=10\n",
    ")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=10\n",
    ")\n",
    "\n",
    "classifier = NaiveBayes()\n",
    "classifier.fit(X_train, y_train)\n",
    "predictions = classifier.predict(X_test)\n",
    "\n",
    "accuracy_score(y_test, predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
